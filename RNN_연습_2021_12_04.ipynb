{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN 연습 2021.12.04",
      "provenance": [],
      "authorship_tag": "ABX9TyOfOsYmSCy1p7o0GR/Px0uL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimheeseo/pytorch-RNN/blob/main/RNN_%EC%97%B0%EC%8A%B5_2021_12_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7t82OBDoeli",
        "outputId": "84ae0b93-cfae-45c6-83bf-77c6f25259e0"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "#출처 : https://justkode.kr/deep-learning/pytorch-rnn\n",
        "sentences = [\"i like dog\", \"i love coffee\", \"i hate milk\", \"you like cat\", \"you love milk\", \"you hate coffee\"]\n",
        "dtype = torch.float\n",
        "\n",
        "\"\"\"\n",
        "Word Processing\n",
        "\"\"\"\n",
        "word_list = list(set(\" \".join(sentences)))\n",
        "print('word_list값',word_list)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_list값 ['c', 'd', 'o', 'i', 'g', 'm', ' ', 'y', 'v', 'f', 'k', 'a', 't', 'e', 'u', 'h', 'l']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5P7r7Szzs4N",
        "outputId": "aa3b908c-c8c9-4ef5-efff-d9fab7ff36c8"
      },
      "source": [
        "word_list = list(set(\" \".join(sentences).split()))\n",
        "print(word_list)\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "print('word_dict값',word_dict)\n",
        "number_dict = {i: w for i, w in enumerate(word_list)}\n",
        "print('number_dict값',number_dict)\n",
        "n_class = len(word_dict)\n",
        "print('n_class값',n_class)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['like', 'dog', 'i', 'you', 'coffee', 'cat', 'love', 'hate', 'milk']\n",
            "word_dict값 {'like': 0, 'dog': 1, 'i': 2, 'you': 3, 'coffee': 4, 'cat': 5, 'love': 6, 'hate': 7, 'milk': 8}\n",
            "number_dict값 {0: 'like', 1: 'dog', 2: 'i', 3: 'you', 4: 'coffee', 5: 'cat', 6: 'love', 7: 'hate', 8: 'milk'}\n",
            "n_class값 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA0Xy3aF4RxV",
        "outputId": "dc1d7fa0-78ac-465c-b467-4ce734569cab"
      },
      "source": [
        "\"\"\"\n",
        "TextRNN Parameter\n",
        "\"\"\"\n",
        "batch_size = len(sentences)\n",
        "print('batch_size값',batch_size)\n",
        "n_step = 2  # 학습 하려고 하는 문장의 길이 - 1\n",
        "n_hidden = 5  # 은닉층 사이즈\n",
        "print('sentences값',sentences)\n",
        "\n",
        "inputtest_batch=[]\n",
        "targettest_batch=[]\n",
        "i=0\n",
        "#다음 for문은 make_batch문의 동작을 보기위해서, 나눠서 표현방법을 본다.\n",
        "for sen in sentences:\n",
        "  print(i)\n",
        "  i=i+1\n",
        "  word=sen.split()\n",
        "  print('word값',word)\n",
        "  input=[word_dict[n] for n in word[:]]\n",
        "  print('input값',input)\n",
        "  target=word_dict[word[-1]]\n",
        "  print('word[-1]값',word[-1])\n",
        "  #print('np.eye(n_class)값',np.eye(n_class))\n",
        "  #print('np.eye(n_class)[input]',np.eye(n_class)[input])\n",
        "  inputtest_batch.append(np.eye(n_class)[input])  # One-Hot Encoding\n",
        "  #print('inputtest_batch값',inputtest_batch)\n",
        "  print('target값',target)\n",
        "  targettest_batch.append(target)\n",
        "\n",
        "print('inputtest_batch값',inputtest_batch)\n",
        "print('targettest_batch값',targettest_batch)\n",
        "\n",
        "#split에 의해 1번째 for문 동작 : 'I like dog'이 sen값이 된다. 그리고 \"word=sen.split()\"에 의해서\n",
        "#word값은 ['I','like']가 되고, input값은 [2,0]이 된다. 우선 2개의 값이 나오는 이유는 [:-1]에 의해서 그런것이고,\n",
        "#만일 [:]이면, ['I','like','dog']가 되고 input값은 [2,0,1]이 되고, 이때 숫자는 앞서 word_dict을 통해 설정한\n",
        "#I는 2, like는 0, dog는 1을 나타내기 때문에, 이와 같은 결과값이 나온다."
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size값 6\n",
            "sentences값 ['i like dog', 'i love coffee', 'i hate milk', 'you like cat', 'you love milk', 'you hate coffee']\n",
            "0\n",
            "word값 ['i', 'like', 'dog']\n",
            "input값 [2, 0, 1]\n",
            "word[-1]값 dog\n",
            "target값 1\n",
            "1\n",
            "word값 ['i', 'love', 'coffee']\n",
            "input값 [2, 6, 4]\n",
            "word[-1]값 coffee\n",
            "target값 4\n",
            "2\n",
            "word값 ['i', 'hate', 'milk']\n",
            "input값 [2, 7, 8]\n",
            "word[-1]값 milk\n",
            "target값 8\n",
            "3\n",
            "word값 ['you', 'like', 'cat']\n",
            "input값 [3, 0, 5]\n",
            "word[-1]값 cat\n",
            "target값 5\n",
            "4\n",
            "word값 ['you', 'love', 'milk']\n",
            "input값 [3, 6, 8]\n",
            "word[-1]값 milk\n",
            "target값 8\n",
            "5\n",
            "word값 ['you', 'hate', 'coffee']\n",
            "input값 [3, 7, 4]\n",
            "word[-1]값 coffee\n",
            "target값 4\n",
            "inputtest_batch값 [array([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 1., 0., 0., 0., 0., 0., 0., 0.]]), array([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "       [0., 0., 0., 0., 1., 0., 0., 0., 0.]]), array([[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 1.]]), array([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 1., 0., 0., 0.]]), array([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 0., 1.]]), array([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "       [0., 0., 0., 0., 1., 0., 0., 0., 0.]])]\n",
            "targettest_batch값 [1, 4, 8, 5, 8, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ69iZxf-PYP"
      },
      "source": [
        "def make_batch(sentences):\n",
        "  input_batch = []\n",
        "  target_batch = []\n",
        "\n",
        "  for sen in sentences:\n",
        "    word = sen.split()\n",
        "    input = [word_dict[n] for n in word[:-1]]\n",
        "    target = word_dict[word[-1]]\n",
        "\n",
        "    input_batch.append(np.eye(n_class)[input])  # One-Hot Encoding\n",
        "    target_batch.append(target)\n",
        "  \n",
        "  return input_batch, target_batch  "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlcGNl745lre"
      },
      "source": [
        "input_batch, target_batch = make_batch(sentences)\n",
        "input_batch = torch.tensor(input_batch, dtype=torch.float32, requires_grad=True)\n",
        "target_batch = torch.tensor(target_batch, dtype=torch.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRbzvF3I5mSL"
      },
      "source": [
        "\"\"\"\n",
        "TextRNN\n",
        "\"\"\"\n",
        "class TextRNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TextRNN, self).__init__()\n",
        "\n",
        "    self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.3)\n",
        "    self.W = nn.Parameter(torch.randn([n_hidden, n_class]).type(dtype))\n",
        "    self.b = nn.Parameter(torch.randn([n_class]).type(dtype))\n",
        "    self.Softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, hidden, X):\n",
        "    X = X.transpose(0, 1)\n",
        "    outputs, hidden = self.rnn(X, hidden)\n",
        "    outputs = outputs[-1]  # 최종 예측 Hidden Layer\n",
        "    model = torch.mm(outputs, self.W) + self.b  # 최종 예측 최종 출력 층\n",
        "    return model\n",
        "\t\n",
        "\n",
        "\"\"\"\n",
        "Training\n",
        "\"\"\"\n",
        "model = TextRNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(500):\n",
        "  hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
        "  output = model(hidden, input_batch)\n",
        "  loss = criterion(output, target_batch)\n",
        "\n",
        "  if (epoch + 1) % 100 == 0:\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "input = [sen.split()[:2] for sen in sentences]\n",
        "\n",
        "hidden = torch.zeros(1, batch_size, n_hidden, requires_grad=True)\n",
        "predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
        "print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9G6kb5U5U5T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}